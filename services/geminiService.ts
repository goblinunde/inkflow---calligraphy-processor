import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });

/**
 * Uses Gemini to restore and clean up the calligraphy before processing.
 * Useful if the source is blurry or has artifacts.
 */
export const restoreImageWithAI = async (base64Image: string): Promise<string> => {
  // Remove data URL prefix for the API
  const cleanBase64 = base64Image.replace(/^data:image\/(png|jpeg|jpg|webp);base64,/, '');
  const mimeType = base64Image.match(/data:image\/(.*?);base64/)?.[1] || 'png';

  try {
    const response = await ai.models.generateContent({
      model: 'gemini-2.5-flash-image',
      contents: {
        parts: [
          {
            inlineData: {
              data: cleanBase64,
              mimeType: `image/${mimeType}`,
            },
          },
          {
            text: `This is an image of Chinese calligraphy (signatures or characters). 
            Please RESTORE this image. 
            1. Keep the exact shape and style of the text.
            2. Make the strokes solid black and high contrast.
            3. Remove any paper texture, noise, or watermarks, making the background pure white.
            4. Fix any broken or jagged edges in the strokes to make them look like smooth ink.
            Return ONLY the restored image.`,
          },
        ],
      },
    });

    // Extract image from response
    // The model usually returns text, but for image tasks it might return an image part if requested implicitly 
    // or we can't force it to return image unless we use image generation/editing specifically.
    // However, 'gemini-2.5-flash-image' is multimodal. Let's see if we can get it to generate an edited version.
    
    // NOTE: Currently, to get an image BACK from Gemini, we typically use an image generation model or an editing flow.
    // Since 2.5-flash-image is primarily for understanding, we might need to rely on the user providing a model that supports outputting images.
    // If the standard GenerateContent doesn't return an image for this model, we fallback or handle the text.
    
    // For this specific requirement (User expertise), let's assume we are using a technique 
    // where we ask it to output a clean version if supported, OR we skip if the model only returns text description.
    
    // Wait, the prompt implies "Convert". The best way with Gemini is to use it to "Edit" or "Generate".
    // Let's assume we use the input as a reference for generation.
    
    // Actually, checking the documentation provided: 
    // "To edit images using the model, you can prompt with text, images or a combination of both."
    // Code snippet provided:
    /*
      const response = await ai.models.generateContent({
        model: 'gemini-2.5-flash-image',
        contents: { parts: [ { inlineData: ... }, { text: '...' } ] }
      });
      // Iterate parts for inlineData
    */

    const candidates = response.candidates;
    if (candidates && candidates.length > 0) {
      const parts = candidates[0].content.parts;
      for (const part of parts) {
        if (part.inlineData && part.inlineData.data) {
           return `data:image/png;base64,${part.inlineData.data}`;
        }
      }
    }
    
    throw new Error("No image generated by AI. It might have returned text description instead.");

  } catch (error) {
    console.error("AI Restoration failed:", error);
    throw error;
  }
};
